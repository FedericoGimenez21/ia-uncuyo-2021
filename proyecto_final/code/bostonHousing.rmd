---
title: "bostonHousing"
output: html_document
date: "2023-04-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r }
library(Metrics)
library(readr)
library(ggplot2)#for visualisation
library(corrplot)#for visualisation of correlation
library(mlbench) 
library(Amelia)
library(plotly)#converting ggplot to plotly
library(reshape2)
library(lattice)
library(caret)
library(caTools)#for splittind data into testing and training data
library(dplyr) #manipulating dataframe
library(mlbench)
library(glmnet)
library(corrplot)
library(catboost)
library(tidyverse)
library(dplyr)
```

```{r dataset}
data("BostonHousing")


housing <- BostonHousing

dim(housing)
str(housing)
```


```{r checking}

#Check for any NA’s in the dataframe.
missmap(housing,col=c('yellow','black'),y.at=1,y.labels='',legend=TRUE)

#Checking for NA and missing values and removing them.
numberOfNA <- length(which(is.na(housing)==T))
numberOfNA
# Remove NA values
housing <- housing %>%
        drop_na()

str(housing) #gives the structure of data


dim(housing)

#Let’s convert ‘chas’ to numeric
housing$chas <- as.numeric(housing$chas)
str(housing$chas)

head(housing)#returns the first six rows of data 
```



```{r analisis}

summary(housing)  #gives the basic statistics of your dataset like mean, median, 1st quartile, 2nd quartile etc.
#In the above image we can see that variable ‘crim’ and ‘b’ take wide range of values.

#Variables ‘crim’, ‘zn’, ‘rm’ and ‘black’ have a large difference between their median and mean which indicates lot of outliers in respective variables.

par(mfrow = c(1, 4))
boxplot(housing$crim, main='crim',col='Sky Blue')
boxplot(housing$zn, main='zn',col='Sky Blue')
boxplot(housing$rm, main='rm',col='Sky Blue')
boxplot(housing$b, main='b',col='Sky Blue')

#As suggested earlier variables ‘crim’, ‘zn’, ‘rm’ and ‘black’ do have a lot of outliers.


#Finding correlation
#Correlation is a statistical measure that suggests the level of linear dependence between two variables that occur in pair. Its value lies between -1 to +1

#If above 0 it means positive correlation i.e. X is directly proportional to Y.
#If below 0 it means negative correlation i.e. X is inversly proportional to Y.
#Value 0 suggests weak relation.
#Usually we would use the function ‘cor’ to find correlation between two variables, but since we have 14 variables here, it is easier to examine the correlation between different varables using corrplot function in library ‘corrplot’.

#Correlation plots are a great way of exploring data and seeing the level of interaction between the variables.


corrplot(cor(housing))

#Looking at the plots, we can see that (except for the diagnal):
#Attributes like ‘tax and rad’, ‘nox and tax’, ‘age and indus’ have positive correlation. Larger darker blue dots suggest storng positive relationship.
#Attributes like ‘dis and nox’, ‘dis and indus’, ‘age and dis’ have negative correlation. Larger darker red dots suggest storng negative relationship.

# Highly correlated variables
correlated <- cor(housing)
highCorr <- findCorrelation(correlated, cutoff=0.70)
highCorr

names(housing[highCorr])
```


```{r split}
#Let’s split the loaded dataset into train and test sets. We will use 75% of the data to train our models and 15% will be used to test the models..

set.seed(120)
split <- sample.split(housing,SplitRatio =0.75)
train <- subset(housing,split==TRUE)
test <- subset(housing,split==FALSE)

dim(housing)
dim(train)

dim(test)

```


```{r linear model}
summary(train)

model <- lm(medv ~ lstat , data = train) # fit a simple linear regression model
summary(model)


pSimpleLineer <- predict(model,test)
postResample(pSimpleLineer,test$medv)


plLinearSimple <-test %>% 
  ggplot(aes(medv,pSimpleLineer)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plLinearSimple)


```

```{r multiple linear model}
#Multiple Linear Regression
#Lets build our model considering that crim,rm,tax,lstat as the major influencers on the target variable.
model2 <- lm(medv ~ crim + rm + tax + lstat , data = train)
summary(model2)
model2

pMultipleLm <- predict(model2,test)
postResample(pMultipleLm,test$medv)


plotMultipleLineer <-test %>% 
  ggplot(aes(medv,pMultipleLm)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of Price') +
  ylab('Predicted value of Price')+
  theme_bw()

ggplotly(plotMultipleLineer)


```



```{r w/o correlated}
# Remove Highly Correlated Variables and build models
# Create new dataset w/o highly correlated variables
corr_data <- train[,-highCorr]

model3 <- lm(medv ~ . , data = corr_data)

pSimpleLineer3 <- predict(model3,test)
postResample(pSimpleLineer3,test$medv)


```

```{r lineer model CV}
#Cross validation
x <- data.matrix(train[,1:13])
y <- train$medv


control <- trainControl(method = "cv",
                        number = 10)

lineerCV <- train(medv~.,
                data = train,
                method = "lm",
                trControl = control )


pLineerCV <- predict(lineerCV,test)
postResample(pLineerCV,test$medv)


#LinearRegression CON CV
plLinearCV <-test %>% 
  ggplot(aes(medv,pLineerCV)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plLinearCV)



```



```{r ridge regression}


ridge <- train(medv~.,
               data = train,
               method = "glmnet",
               tuneGrid = expand.grid(alpha = 0,
                                      lambda = seq(0.0001,1,length=50)),
               trControl = control )


pRidge <- predict(ridge,test)
postResample(pRidge,test$medv)


plRidge <-test %>% 
  ggplot(aes(medv,pRidge)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plRidge)



```



```{r lasso regression}


lasso <- train(medv~.,
               data = train,
               method = "glmnet",
               tuneGrid = expand.grid(alpha = 1,
                                      lambda = seq(0.0001,1,length=50)),
               trControl = control )


# Test RMSE
pLasso <- predict(lasso,test)

postResample(pLasso,test$medv)


#Lasoo 

plLasso <-test %>% 
  ggplot(aes(medv,pLasso)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plLasso)




```



```{r catboost}

#Separate x and y of train and test dataset, which will very useful when we using this in the catboost package.
y_train <- unlist(train[c('medv')])
X_train <- train %>% select(-medv)
y_valid <- unlist(test[c('medv')])
X_valid <- test %>% select(-medv)

#Convert the train and test dataset to catboost specific format using the load_pool function by mentioning x and y of both train and test.
train_pool <- catboost.load_pool(data = X_train, label = y_train)
test_pool <- catboost.load_pool(data = X_valid, label = y_valid)

#Create an input params for the CatBoost regression.
params <- list(iterations=500,
               learning_rate=0.01,
               depth=10,
               loss_function='RMSE',
               eval_metric='RMSE',
               random_seed = 55,
               od_type='Iter',
               metric_period = 50,
               od_wait=20,
               use_best_model=TRUE)


#Iterations- The maximum number of trees that can be built when solving machine learning problems.
#Build a model using the catboost train function. Pass the train dataset and parameters to the catboost train function.
modelCatboost <- catboost.train(learn_pool = train_pool,params = params)

#predict
y_pred=catboost.predict(modelCatboost,test_pool)

#calculate error metrics
catboostMetrics <- postResample(y_pred,test$medv)
catboostMetrics

#Catboost
plCatboost <-test %>% 
  ggplot(aes(medv,y_pred)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plCatboost)


#Catboost without correlated variables

corr_dataTEST <- test[,-highCorr]
#corr_data
train_pool2 <- catboost.load_pool(data = corr_data, label = y_train)
test_pool2 <- catboost.load_pool(data = corr_dataTEST, label = y_valid)
#Build a model
modelCatboost2 <- catboost.train(learn_pool = train_pool2,params = params)

#predict
y_pred2=catboost.predict(modelCatboost2,test_pool2)
catboostMetrics2 <- postResample(y_pred2,test$medv)
catboostMetrics2

#plot
plCatboost2 <-test %>% 
  ggplot(aes(medv,y_pred2)) +
  geom_point(alpha=0.5) + 
  stat_smooth(aes(colour='black')) +
  xlab('Actual value of medv') +
  ylab('Predicted value of medv')+
  theme_bw()

ggplotly(plCatboost2)
```


